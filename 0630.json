[
    {
        "commit_message": "refactor generate slug",
        "added_files": {
            "writing_helper/llm.py": "import os\nfrom typing import List, Dict, Optional, Any, Generator\nfrom openai import OpenAI\nfrom openai.types.chat import ChatCompletion, ChatCompletionChunk\n\n\nclass LLMClient:\n    \"\"\"\n    大模型调用客户端，基于 OpenAI SDK 封装 Cloudflare AI Gateway 调用\n    \"\"\"\n    \n    def __init__(self):\n        self.api_key = self._get_api_key()\n        self.base_url = \"https://gateway.ai.cloudflare.com/v1/d86fcdf114d9c154ee61f5b601d8cae3/my-agent/compat\"\n        self.client = OpenAI(\n            base_url=self.base_url,\n            api_key=self.api_key\n        )\n    \n    def _get_api_key(self) -> str:\n        \"\"\"从环境变量获取API密钥\"\"\"\n        api_key = os.getenv(\"GOOGLE_AI_STUDIO_API_KEY\")\n        if not api_key:\n            raise ValueError(\"环境变量 GOOGLE_AI_STUDIO_API_KEY 未设置\")\n        return api_key\n    \n    def chat_completion(\n        self,\n        messages: List[Dict[str, str]],\n        model: str = \"google-ai-studio/gemini-2.5-flash\",\n        temperature: float = 0.7,\n        max_tokens: Optional[int] = None,\n        stream: bool = False,\n        **kwargs\n    ) -> ChatCompletion:\n        \"\"\"\n        调用大模型进行对话完成\n        \n        Args:\n            messages: 对话消息列表，格式为 [{\"role\": \"user|assistant|system\", \"content\": \"消息内容\"}]\n            model: 模型名称，默认为 gemini-2.5-flash\n            temperature: 温度参数，控制回复的随机性，0-1之间\n            max_tokens: 最大token数量\n            stream: 是否使用流式输出\n            **kwargs: 其他可选参数\n            \n        Returns:\n            ChatCompletion: OpenAI 格式的响应对象\n            \n        Raises:\n            Exception: API调用异常\n        \"\"\"\n        try:\n            # 构建参数字典\n            params = {\n                \"model\": model,\n                \"messages\": messages,\n                \"temperature\": temperature,\n                \"stream\": stream,\n                **kwargs\n            }\n            \n            if max_tokens is not None:\n                params[\"max_tokens\"] = max_tokens\n            \n            response = self.client.chat.completions.create(**params)\n            return response\n            \n        except Exception as e:\n            raise Exception(f\"LLM API调用失败: {str(e)}\")\n    \n    def chat_completion_stream(\n        self,\n        messages: List[Dict[str, str]],\n        model: str = \"google-ai-studio/gemini-2.5-flash\",\n        temperature: float = 0.7,\n        max_tokens: Optional[int] = None,\n        **kwargs\n    ) -> Generator[ChatCompletionChunk, None, None]:\n        \"\"\"\n        流式调用大模型进行对话完成\n        \n        Args:\n            messages: 对话消息列表\n            model: 模型名称\n            temperature: 温度参数\n            max_tokens: 最大token数量\n            **kwargs: 其他可选参数\n            \n        Yields:\n            ChatCompletionChunk: 流式响应片段\n        \"\"\"\n        try:\n            params = {\n                \"model\": model,\n                \"messages\": messages,\n                \"temperature\": temperature,\n                \"stream\": True,\n                **kwargs\n            }\n            \n            if max_tokens is not None:\n                params[\"max_tokens\"] = max_tokens\n            \n            stream = self.client.chat.completions.create(**params)\n            for chunk in stream:\n                yield chunk\n                \n        except Exception as e:\n            raise Exception(f\"LLM 流式API调用失败: {str(e)}\")\n    \n    def chat_simple(\n        self,\n        user_message: str,\n        system_message: Optional[str] = None,\n        model: str = \"google-ai-studio/gemini-2.5-flash\",\n        temperature: float = 0.7,\n        max_tokens: Optional[int] = None,\n        **kwargs\n    ) -> str:\n        \"\"\"\n        简单的单轮对话接口，只支持非流式输出\n        \n        Args:\n            user_message: 用户消息\n            system_message: 系统消息（可选）\n            model: 模型名称\n            temperature: 温度参数\n            max_tokens: 最大token数量\n            **kwargs: 其他参数\n            \n        Returns:\n            str: 助手的回复文本\n        \"\"\"\n        messages = []\n        \n        if system_message:\n            messages.append({\"role\": \"system\", \"content\": system_message})\n        \n        messages.append({\"role\": \"user\", \"content\": user_message})\n        \n        # 强制非流式调用\n        response = self.chat_completion(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            stream=False,\n            **kwargs\n        )\n        \n        # 提取回复文本\n        if response.choices and len(response.choices) > 0:\n            return response.choices[0].message.content or \"\"\n        else:\n            raise ValueError(\"API响应格式异常，未找到回复内容\")\n    \n\n\n\ndef create_llm_client() -> LLMClient:\n    \"\"\"创建LLM客户端实例\"\"\"\n    return LLMClient()\n\n\nif __name__ == \"__main__\":\n    # 测试代码\n    try:\n        # 创建客户端\n        client = create_llm_client()\n        \n        # 测试简单对话（chat_simple）\n        print(\"=== 测试简单对话（chat_simple）===\")\n        response = client.chat_simple(\"你是谁？\")\n        print(f\"回复: {response}\")\n        \n        # 测试带系统消息的简单对话\n        print(\"\\n=== 测试带系统消息的简单对话 ===\")\n        system_msg = \"你是一个专业的Python编程助手，请简洁地回答问题。\"\n        response = client.chat_simple(\"什么是列表推导式？\", system_message=system_msg)\n        print(f\"回复: {response}\")\n        \n        # 测试多轮对话（chat_completion）\n        print(\"\\n=== 测试多轮对话（chat_completion）===\")\n        messages = [\n            {\"role\": \"system\", \"content\": \"你是一个友好的助手\"},\n            {\"role\": \"user\", \"content\": \"请用一句话介绍一下Python\"},\n            {\"role\": \"assistant\", \"content\": \"Python是一种简洁、易学且功能强大的编程语言。\"},\n            {\"role\": \"user\", \"content\": \"它主要用在哪些领域？\"}\n        ]\n        \n        full_response = client.chat_completion(messages)\n        print(f\"模型: {full_response.model}\")\n        print(f\"回复: {full_response.choices[0].message.content}\")\n        print(f\"Token使用: {full_response.usage}\")\n        \n        # 测试流式输出（chat_completion_stream）\n        print(\"\\n=== 测试流式输出（chat_completion_stream）===\")\n        messages = [\n            {\"role\": \"user\", \"content\": \"请说一个编程笑话\"}\n        ]\n        \n        print(\"流式回复: \", end=\"\", flush=True)\n        for chunk in client.chat_completion_stream(messages):\n            if chunk.choices and len(chunk.choices) > 0:\n                delta = chunk.choices[0].delta\n                if delta.content:\n                    print(delta.content, end=\"\", flush=True)\n        print()  # 换行\n        \n    except Exception as e:\n        print(f\"测试过程中出现错误: {str(e)}\")\n        print(\"请确保已正确设置环境变量 GOOGLE_AI_STUDIO_API_KEY\")\n        print(\"并且安装了 openai 库: pip install openai\")",
            "writing_helper/slug_generator.py": "#!/usr/bin/env python3\n\"\"\"\nSlug Generator - 博客文章slug生成器\n\"\"\"\n\nimport re\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport frontmatter\n\nfrom llm import create_llm_client\n\n\ndef find_workdir_modified_blog_files(root_dir: Path, blog_dir: Path) -> list[Path]:\n    \"\"\"\n    查找工作区中修改的博客文件\n    \n    Args:\n        root_dir: 项目根目录\n        blog_dir: 博客目录路径\n        \n    Returns:\n        list[Path]: 工作区中修改的 .md 文件列表\n        \n    Raises:\n        subprocess.CalledProcessError: 如果无法获取git状态\n    \"\"\"\n    result = subprocess.run(\n        [\"git\", \"status\", \"--porcelain\"],\n        cwd=root_dir,\n        capture_output=True,\n        text=True,\n        check=True\n    )\n    \n    # 使用正则表达式直接匹配博客目录下的.md文件\n    # 匹配模式：任意git状态 + src/content/blog/xxx.md\n    blog_pattern = re.compile(r'src/content/blog/.*\\.md')\n    modified_files = []\n    \n    for line in result.stdout.strip().split('\\n'):\n        if not line:\n            continue\n        \n        # 使用正则表达式提取博客文件路径\n        file_path_match = blog_pattern.search(line)\n        if file_path_match:\n            file_path = file_path_match.group()\n            full_path = root_dir / file_path\n            if full_path.exists():\n                modified_files.append(full_path)\n    \n    return modified_files\n\n\ndef find_last_commit_blog_files(root_dir: Path, blog_dir: Path) -> list[Path]:\n    \"\"\"\n    查找最近一次提交中修改的博客文件\n    \n    Args:\n        root_dir: 项目根目录\n        blog_dir: 博客目录路径\n        \n    Returns:\n        list[Path]: 最近提交中修改的 .md 文件列表\n        \n    Raises:\n        subprocess.CalledProcessError: 如果无法获取提交信息\n    \"\"\"\n    # 获取最近一次提交中修改的文件\n    result = subprocess.run(\n        [\"git\", \"diff\", \"--name-only\", \"HEAD~1\", \"HEAD\"],\n        cwd=root_dir,\n        capture_output=True,\n        text=True,\n        check=True\n    )\n    \n    # 使用正则表达式直接匹配博客目录下的.md文件\n    blog_pattern = re.compile(r'src/content/blog/.*\\.md')\n    committed_files = []\n    \n    for line in result.stdout.strip().split('\\n'):\n        if not line:\n            continue\n        \n        # 使用正则表达式提取博客文件路径\n        file_path_match = blog_pattern.search(line)\n        if file_path_match:\n            file_path = file_path_match.group()\n            full_path = root_dir / file_path\n            if full_path.exists():  # 确保文件仍然存在\n                committed_files.append(full_path)\n    \n    return committed_files\n\n\ndef find_modified_blog_file() -> Path:\n    \"\"\"\n    找到正在修改的博客文件\n    \n    优先级：\n    1. 工作区中修改的 .md 文件（未提交的内容）\n    2. 最近一次提交中的 .md 文件（如果只有一个）\n    \n    Returns:\n        Path: 修改的博客文件路径\n        \n    Raises:\n        SystemExit: 如果找不到文件或找到多个文件\n    \"\"\"\n    # 获取项目根目录\n    root_dir = Path(__file__).parent.parent\n    blog_dir = root_dir / \"src\" / \"content\" / \"blog\"\n    \n    # 1. 优先检查工作区状态（未提交的修改）\n    try:\n        modified_files = find_workdir_modified_blog_files(root_dir, blog_dir)\n    except subprocess.CalledProcessError as e:\n        print(f\"错误：无法获取git状态: {e}\")\n        sys.exit(1)\n    \n    # 如果工作区有修改的文件，优先使用\n    if len(modified_files) == 1:\n        return modified_files[0]\n    elif len(modified_files) > 1:\n        print(f\"错误：找到多个修改的 .md 文件，请确保只修改一个文件：\")\n        for file in modified_files:\n            print(f\"  - {file.relative_to(root_dir)}\")\n        sys.exit(1)\n    \n    # 2. 如果工作区没有修改，检查最近一次提交\n    print(\"📝 工作区无修改文件，检查最近一次提交...\")\n    try:\n        committed_files = find_last_commit_blog_files(root_dir, blog_dir)\n    except subprocess.CalledProcessError as e:\n        print(f\"错误：无法获取最近提交的文件: {e}\")\n        sys.exit(1)\n    \n    if len(committed_files) == 0:\n        print(\"错误：在工作区和最近一次提交中都没有找到博客 .md 文件\")\n        sys.exit(1)\n    elif len(committed_files) == 1:\n        print(f\"✅ 使用最近提交中的文件: {committed_files[0].name}\")\n        return committed_files[0]\n    else:\n        print(f\"错误：最近一次提交中找到多个 .md 文件：\")\n        for file in committed_files:\n            print(f\"  - {file.relative_to(root_dir)}\")\n        sys.exit(1)\n\n\ndef extract_title_from_markdown(file_path: Path) -> str:\n    \"\"\"\n    从markdown文件提取标题\n    \n    Args:\n        file_path: markdown文件路径\n        \n    Returns:\n        str: 文章标题\n        \n    Raises:\n        SystemExit: 如果无法解析文件或缺少标题\n    \"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            post = frontmatter.load(f)\n    except Exception as e:\n        print(f\"错误：无法读取文件 {file_path}: {e}\")\n        sys.exit(1)\n    \n    title = post.metadata.get('title')\n    if not title:\n        print(f\"错误：文件 {file_path} 的frontmatter中缺少 'title' 字段\")\n        sys.exit(1)\n    \n    return str(title)\n\n\ndef generate_slug_with_llm(title: str) -> str:\n    \"\"\"\n    调用LLM生成slug\n    \n    Args:\n        title: 文章标题\n        \n    Returns:\n        str: 生成的slug\n        \n    Raises:\n        SystemExit: 如果LLM调用失败\n    \"\"\"\n    prompt = f\"Generate a URL slug for this article title: {title}\\n\\nRequirements:\\n- Only use lowercase letters, numbers, and hyphens (-)\\n- Maximum 50 characters\\n- Be concise and reflect the main topic\\n- Return only the slug string\\n\\nSlug:\"\n\n    try:\n        client = create_llm_client()\n        \n        # 调用LLM生成slug\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        response_obj = client.chat_completion(\n            messages=messages,\n            temperature=0.3  # 降低随机性，确保输出稳定\n        )\n        \n        response = response_obj.choices[0].message.content or \"\"\n        \n        # 清理响应，去除可能的前缀和后缀\n        slug = response.strip()\n        if slug.startswith('slug:'):\n            slug = slug[5:].strip()\n        if slug.startswith('Slug:'):\n            slug = slug[5:].strip()\n        \n        # 验证生成的slug格式\n        if not slug:\n            print(f\"错误：LLM返回空响应\")\n            sys.exit(1)\n        \n        if not all(c.islower() or c.isdigit() or c == '-' for c in slug):\n            print(f\"错误：生成的slug包含非法字符: {slug}\")\n            print(f\"非法字符: {[c for c in slug if not (c.islower() or c.isdigit() or c == '-')]}\")\n            sys.exit(1)\n            \n        return slug\n        \n    except Exception as e:\n        print(f\"错误：LLM调用失败: {e}\")\n        import traceback\n        traceback.print_exc()\n        sys.exit(1)\n\n\ndef rename_blog_file(old_path: Path, slug: str) -> Path:\n    \"\"\"\n    根据slug重命名文件\n    \n    Args:\n        old_path: 原文件路径\n        slug: 新的slug\n        \n    Returns:\n        Path: 新文件路径\n        \n    Raises:\n        SystemExit: 如果重命名失败\n    \"\"\"\n    new_filename = f\"{slug}.md\"\n    new_path = old_path.parent / new_filename\n    \n    # 检查新文件名是否已存在\n    if new_path.exists():\n        print(f\"错误：目标文件已存在: {new_path}\")\n        sys.exit(1)\n    \n    try:\n        old_path.rename(new_path)\n        return new_path\n    except Exception as e:\n        print(f\"错误：重命名文件失败: {e}\")\n        sys.exit(1)\n\n\ndef generate_blog_slug() -> None:\n    \"\"\"\n    生成博客文章slug的主函数\n    \"\"\"\n    print(\"🚀 开始自动生成文章slug...\")\n    \n    # 1. 找到正在修改的博客文件\n    print(\"📁 检测正在修改的博客文件...\")\n    blog_file = find_modified_blog_file()\n    print(f\"✅ 找到文件: {blog_file.name}\")\n    \n    # 2. 提取文章标题\n    print(\"📝 提取文章标题...\")\n    title = extract_title_from_markdown(blog_file)\n    print(f\"✅ 文章标题: {title}\")\n    \n    # 3. 生成slug\n    print(\"🤖 调用LLM生成slug...\")\n    slug = generate_slug_with_llm(title)\n    print(f\"✅ 生成的slug: {slug}\")\n    \n    # 4. 重命名文件\n    print(\"📝 重命名文件...\")\n    new_path = rename_blog_file(blog_file, slug)\n    print(f\"✅ 文件已重命名: {blog_file.name} -> {new_path.name}\")\n    \n    print(\"🎉 slug生成完成！\")\n\n\nif __name__ == \"__main__\":\n    generate_blog_slug()"
        },
        "modified_files": {
            "writing_helper/main.py": "#!/usr/bin/env python3\n\"\"\"\nWriting Helper - 基于大模型的写作助手\n\n主入口文件，根据命令行参数调用对应的功能模块\n\"\"\"\n\nimport sys\nfrom pathlib import Path\n\n# 添加当前目录到Python路径，以便导入模块\ncurrent_dir = Path(__file__).parent\nsys.path.append(str(current_dir))\n\nfrom slug_generator import generate_blog_slug\n\n\ndef main():\n    \"\"\"主函数 - 写作助手入口\"\"\"\n    # 目前只有slug生成功能，直接调用\n    # 未来可以根据命令行参数扩展其他功能\n    generate_blog_slug()\n\n\nif __name__ == \"__main__\":\n    main()"
        },
        "deleted_files": [
            "writing_helper/writing_helper/__init__.py",
            "writing_helper/writing_helper/llm.py",
            "writing_helper/writing_helper/main.py"
        ]
    }
]
