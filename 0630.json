[
    {
        "commit_message": "refactor generate slug",
        "added_files": {
            "writing_helper/llm.py": "import os\nfrom typing import List, Dict, Optional, Any, Generator\nfrom openai import OpenAI\nfrom openai.types.chat import ChatCompletion, ChatCompletionChunk\n\n\nclass LLMClient:\n    \"\"\"\n    å¤§æ¨¡å‹è°ƒç”¨å®¢æˆ·ç«¯ï¼ŒåŸºäº OpenAI SDK å°è£… Cloudflare AI Gateway è°ƒç”¨\n    \"\"\"\n    \n    def __init__(self):\n        self.api_key = self._get_api_key()\n        self.base_url = \"https://gateway.ai.cloudflare.com/v1/d86fcdf114d9c154ee61f5b601d8cae3/my-agent/compat\"\n        self.client = OpenAI(\n            base_url=self.base_url,\n            api_key=self.api_key\n        )\n    \n    def _get_api_key(self) -> str:\n        \"\"\"ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥\"\"\"\n        api_key = os.getenv(\"GOOGLE_AI_STUDIO_API_KEY\")\n        if not api_key:\n            raise ValueError(\"ç¯å¢ƒå˜é‡ GOOGLE_AI_STUDIO_API_KEY æœªè®¾ç½®\")\n        return api_key\n    \n    def chat_completion(\n        self,\n        messages: List[Dict[str, str]],\n        model: str = \"google-ai-studio/gemini-2.5-flash\",\n        temperature: float = 0.7,\n        max_tokens: Optional[int] = None,\n        stream: bool = False,\n        **kwargs\n    ) -> ChatCompletion:\n        \"\"\"\n        è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œå¯¹è¯å®Œæˆ\n        \n        Args:\n            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{\"role\": \"user|assistant|system\", \"content\": \"æ¶ˆæ¯å†…å®¹\"}]\n            model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º gemini-2.5-flash\n            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶å›å¤çš„éšæœºæ€§ï¼Œ0-1ä¹‹é—´\n            max_tokens: æœ€å¤§tokenæ•°é‡\n            stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º\n            **kwargs: å…¶ä»–å¯é€‰å‚æ•°\n            \n        Returns:\n            ChatCompletion: OpenAI æ ¼å¼çš„å“åº”å¯¹è±¡\n            \n        Raises:\n            Exception: APIè°ƒç”¨å¼‚å¸¸\n        \"\"\"\n        try:\n            # æ„å»ºå‚æ•°å­—å…¸\n            params = {\n                \"model\": model,\n                \"messages\": messages,\n                \"temperature\": temperature,\n                \"stream\": stream,\n                **kwargs\n            }\n            \n            if max_tokens is not None:\n                params[\"max_tokens\"] = max_tokens\n            \n            response = self.client.chat.completions.create(**params)\n            return response\n            \n        except Exception as e:\n            raise Exception(f\"LLM APIè°ƒç”¨å¤±è´¥: {str(e)}\")\n    \n    def chat_completion_stream(\n        self,\n        messages: List[Dict[str, str]],\n        model: str = \"google-ai-studio/gemini-2.5-flash\",\n        temperature: float = 0.7,\n        max_tokens: Optional[int] = None,\n        **kwargs\n    ) -> Generator[ChatCompletionChunk, None, None]:\n        \"\"\"\n        æµå¼è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œå¯¹è¯å®Œæˆ\n        \n        Args:\n            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨\n            model: æ¨¡å‹åç§°\n            temperature: æ¸©åº¦å‚æ•°\n            max_tokens: æœ€å¤§tokenæ•°é‡\n            **kwargs: å…¶ä»–å¯é€‰å‚æ•°\n            \n        Yields:\n            ChatCompletionChunk: æµå¼å“åº”ç‰‡æ®µ\n        \"\"\"\n        try:\n            params = {\n                \"model\": model,\n                \"messages\": messages,\n                \"temperature\": temperature,\n                \"stream\": True,\n                **kwargs\n            }\n            \n            if max_tokens is not None:\n                params[\"max_tokens\"] = max_tokens\n            \n            stream = self.client.chat.completions.create(**params)\n            for chunk in stream:\n                yield chunk\n                \n        except Exception as e:\n            raise Exception(f\"LLM æµå¼APIè°ƒç”¨å¤±è´¥: {str(e)}\")\n    \n    def chat_simple(\n        self,\n        user_message: str,\n        system_message: Optional[str] = None,\n        model: str = \"google-ai-studio/gemini-2.5-flash\",\n        temperature: float = 0.7,\n        max_tokens: Optional[int] = None,\n        **kwargs\n    ) -> str:\n        \"\"\"\n        ç®€å•çš„å•è½®å¯¹è¯æ¥å£ï¼Œåªæ”¯æŒéæµå¼è¾“å‡º\n        \n        Args:\n            user_message: ç”¨æˆ·æ¶ˆæ¯\n            system_message: ç³»ç»Ÿæ¶ˆæ¯ï¼ˆå¯é€‰ï¼‰\n            model: æ¨¡å‹åç§°\n            temperature: æ¸©åº¦å‚æ•°\n            max_tokens: æœ€å¤§tokenæ•°é‡\n            **kwargs: å…¶ä»–å‚æ•°\n            \n        Returns:\n            str: åŠ©æ‰‹çš„å›å¤æ–‡æœ¬\n        \"\"\"\n        messages = []\n        \n        if system_message:\n            messages.append({\"role\": \"system\", \"content\": system_message})\n        \n        messages.append({\"role\": \"user\", \"content\": user_message})\n        \n        # å¼ºåˆ¶éæµå¼è°ƒç”¨\n        response = self.chat_completion(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            stream=False,\n            **kwargs\n        )\n        \n        # æå–å›å¤æ–‡æœ¬\n        if response.choices and len(response.choices) > 0:\n            return response.choices[0].message.content or \"\"\n        else:\n            raise ValueError(\"APIå“åº”æ ¼å¼å¼‚å¸¸ï¼Œæœªæ‰¾åˆ°å›å¤å†…å®¹\")\n    \n\n\n\ndef create_llm_client() -> LLMClient:\n    \"\"\"åˆ›å»ºLLMå®¢æˆ·ç«¯å®ä¾‹\"\"\"\n    return LLMClient()\n\n\nif __name__ == \"__main__\":\n    # æµ‹è¯•ä»£ç \n    try:\n        # åˆ›å»ºå®¢æˆ·ç«¯\n        client = create_llm_client()\n        \n        # æµ‹è¯•ç®€å•å¯¹è¯ï¼ˆchat_simpleï¼‰\n        print(\"=== æµ‹è¯•ç®€å•å¯¹è¯ï¼ˆchat_simpleï¼‰===\")\n        response = client.chat_simple(\"ä½ æ˜¯è°ï¼Ÿ\")\n        print(f\"å›å¤: {response}\")\n        \n        # æµ‹è¯•å¸¦ç³»ç»Ÿæ¶ˆæ¯çš„ç®€å•å¯¹è¯\n        print(\"\\n=== æµ‹è¯•å¸¦ç³»ç»Ÿæ¶ˆæ¯çš„ç®€å•å¯¹è¯ ===\")\n        system_msg = \"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonç¼–ç¨‹åŠ©æ‰‹ï¼Œè¯·ç®€æ´åœ°å›ç­”é—®é¢˜ã€‚\"\n        response = client.chat_simple(\"ä»€ä¹ˆæ˜¯åˆ—è¡¨æ¨å¯¼å¼ï¼Ÿ\", system_message=system_msg)\n        print(f\"å›å¤: {response}\")\n        \n        # æµ‹è¯•å¤šè½®å¯¹è¯ï¼ˆchat_completionï¼‰\n        print(\"\\n=== æµ‹è¯•å¤šè½®å¯¹è¯ï¼ˆchat_completionï¼‰===\")\n        messages = [\n            {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹\"},\n            {\"role\": \"user\", \"content\": \"è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä¸€ä¸‹Python\"},\n            {\"role\": \"assistant\", \"content\": \"Pythonæ˜¯ä¸€ç§ç®€æ´ã€æ˜“å­¦ä¸”åŠŸèƒ½å¼ºå¤§çš„ç¼–ç¨‹è¯­è¨€ã€‚\"},\n            {\"role\": \"user\", \"content\": \"å®ƒä¸»è¦ç”¨åœ¨å“ªäº›é¢†åŸŸï¼Ÿ\"}\n        ]\n        \n        full_response = client.chat_completion(messages)\n        print(f\"æ¨¡å‹: {full_response.model}\")\n        print(f\"å›å¤: {full_response.choices[0].message.content}\")\n        print(f\"Tokenä½¿ç”¨: {full_response.usage}\")\n        \n        # æµ‹è¯•æµå¼è¾“å‡ºï¼ˆchat_completion_streamï¼‰\n        print(\"\\n=== æµ‹è¯•æµå¼è¾“å‡ºï¼ˆchat_completion_streamï¼‰===\")\n        messages = [\n            {\"role\": \"user\", \"content\": \"è¯·è¯´ä¸€ä¸ªç¼–ç¨‹ç¬‘è¯\"}\n        ]\n        \n        print(\"æµå¼å›å¤: \", end=\"\", flush=True)\n        for chunk in client.chat_completion_stream(messages):\n            if chunk.choices and len(chunk.choices) > 0:\n                delta = chunk.choices[0].delta\n                if delta.content:\n                    print(delta.content, end=\"\", flush=True)\n        print()  # æ¢è¡Œ\n        \n    except Exception as e:\n        print(f\"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}\")\n        print(\"è¯·ç¡®ä¿å·²æ­£ç¡®è®¾ç½®ç¯å¢ƒå˜é‡ GOOGLE_AI_STUDIO_API_KEY\")\n        print(\"å¹¶ä¸”å®‰è£…äº† openai åº“: pip install openai\")",
            "writing_helper/slug_generator.py": "#!/usr/bin/env python3\n\"\"\"\nSlug Generator - åšå®¢æ–‡ç« slugç”Ÿæˆå™¨\n\"\"\"\n\nimport re\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport frontmatter\n\nfrom llm import create_llm_client\n\n\ndef find_workdir_modified_blog_files(root_dir: Path, blog_dir: Path) -> list[Path]:\n    \"\"\"\n    æŸ¥æ‰¾å·¥ä½œåŒºä¸­ä¿®æ”¹çš„åšå®¢æ–‡ä»¶\n    \n    Args:\n        root_dir: é¡¹ç›®æ ¹ç›®å½•\n        blog_dir: åšå®¢ç›®å½•è·¯å¾„\n        \n    Returns:\n        list[Path]: å·¥ä½œåŒºä¸­ä¿®æ”¹çš„ .md æ–‡ä»¶åˆ—è¡¨\n        \n    Raises:\n        subprocess.CalledProcessError: å¦‚æœæ— æ³•è·å–gitçŠ¶æ€\n    \"\"\"\n    result = subprocess.run(\n        [\"git\", \"status\", \"--porcelain\"],\n        cwd=root_dir,\n        capture_output=True,\n        text=True,\n        check=True\n    )\n    \n    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç›´æ¥åŒ¹é…åšå®¢ç›®å½•ä¸‹çš„.mdæ–‡ä»¶\n    # åŒ¹é…æ¨¡å¼ï¼šä»»æ„gitçŠ¶æ€ + src/content/blog/xxx.md\n    blog_pattern = re.compile(r'src/content/blog/.*\\.md')\n    modified_files = []\n    \n    for line in result.stdout.strip().split('\\n'):\n        if not line:\n            continue\n        \n        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–åšå®¢æ–‡ä»¶è·¯å¾„\n        file_path_match = blog_pattern.search(line)\n        if file_path_match:\n            file_path = file_path_match.group()\n            full_path = root_dir / file_path\n            if full_path.exists():\n                modified_files.append(full_path)\n    \n    return modified_files\n\n\ndef find_last_commit_blog_files(root_dir: Path, blog_dir: Path) -> list[Path]:\n    \"\"\"\n    æŸ¥æ‰¾æœ€è¿‘ä¸€æ¬¡æäº¤ä¸­ä¿®æ”¹çš„åšå®¢æ–‡ä»¶\n    \n    Args:\n        root_dir: é¡¹ç›®æ ¹ç›®å½•\n        blog_dir: åšå®¢ç›®å½•è·¯å¾„\n        \n    Returns:\n        list[Path]: æœ€è¿‘æäº¤ä¸­ä¿®æ”¹çš„ .md æ–‡ä»¶åˆ—è¡¨\n        \n    Raises:\n        subprocess.CalledProcessError: å¦‚æœæ— æ³•è·å–æäº¤ä¿¡æ¯\n    \"\"\"\n    # è·å–æœ€è¿‘ä¸€æ¬¡æäº¤ä¸­ä¿®æ”¹çš„æ–‡ä»¶\n    result = subprocess.run(\n        [\"git\", \"diff\", \"--name-only\", \"HEAD~1\", \"HEAD\"],\n        cwd=root_dir,\n        capture_output=True,\n        text=True,\n        check=True\n    )\n    \n    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç›´æ¥åŒ¹é…åšå®¢ç›®å½•ä¸‹çš„.mdæ–‡ä»¶\n    blog_pattern = re.compile(r'src/content/blog/.*\\.md')\n    committed_files = []\n    \n    for line in result.stdout.strip().split('\\n'):\n        if not line:\n            continue\n        \n        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–åšå®¢æ–‡ä»¶è·¯å¾„\n        file_path_match = blog_pattern.search(line)\n        if file_path_match:\n            file_path = file_path_match.group()\n            full_path = root_dir / file_path\n            if full_path.exists():  # ç¡®ä¿æ–‡ä»¶ä»ç„¶å­˜åœ¨\n                committed_files.append(full_path)\n    \n    return committed_files\n\n\ndef find_modified_blog_file() -> Path:\n    \"\"\"\n    æ‰¾åˆ°æ­£åœ¨ä¿®æ”¹çš„åšå®¢æ–‡ä»¶\n    \n    ä¼˜å…ˆçº§ï¼š\n    1. å·¥ä½œåŒºä¸­ä¿®æ”¹çš„ .md æ–‡ä»¶ï¼ˆæœªæäº¤çš„å†…å®¹ï¼‰\n    2. æœ€è¿‘ä¸€æ¬¡æäº¤ä¸­çš„ .md æ–‡ä»¶ï¼ˆå¦‚æœåªæœ‰ä¸€ä¸ªï¼‰\n    \n    Returns:\n        Path: ä¿®æ”¹çš„åšå®¢æ–‡ä»¶è·¯å¾„\n        \n    Raises:\n        SystemExit: å¦‚æœæ‰¾ä¸åˆ°æ–‡ä»¶æˆ–æ‰¾åˆ°å¤šä¸ªæ–‡ä»¶\n    \"\"\"\n    # è·å–é¡¹ç›®æ ¹ç›®å½•\n    root_dir = Path(__file__).parent.parent\n    blog_dir = root_dir / \"src\" / \"content\" / \"blog\"\n    \n    # 1. ä¼˜å…ˆæ£€æŸ¥å·¥ä½œåŒºçŠ¶æ€ï¼ˆæœªæäº¤çš„ä¿®æ”¹ï¼‰\n    try:\n        modified_files = find_workdir_modified_blog_files(root_dir, blog_dir)\n    except subprocess.CalledProcessError as e:\n        print(f\"é”™è¯¯ï¼šæ— æ³•è·å–gitçŠ¶æ€: {e}\")\n        sys.exit(1)\n    \n    # å¦‚æœå·¥ä½œåŒºæœ‰ä¿®æ”¹çš„æ–‡ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨\n    if len(modified_files) == 1:\n        return modified_files[0]\n    elif len(modified_files) > 1:\n        print(f\"é”™è¯¯ï¼šæ‰¾åˆ°å¤šä¸ªä¿®æ”¹çš„ .md æ–‡ä»¶ï¼Œè¯·ç¡®ä¿åªä¿®æ”¹ä¸€ä¸ªæ–‡ä»¶ï¼š\")\n        for file in modified_files:\n            print(f\"  - {file.relative_to(root_dir)}\")\n        sys.exit(1)\n    \n    # 2. å¦‚æœå·¥ä½œåŒºæ²¡æœ‰ä¿®æ”¹ï¼Œæ£€æŸ¥æœ€è¿‘ä¸€æ¬¡æäº¤\n    print(\"ğŸ“ å·¥ä½œåŒºæ— ä¿®æ”¹æ–‡ä»¶ï¼Œæ£€æŸ¥æœ€è¿‘ä¸€æ¬¡æäº¤...\")\n    try:\n        committed_files = find_last_commit_blog_files(root_dir, blog_dir)\n    except subprocess.CalledProcessError as e:\n        print(f\"é”™è¯¯ï¼šæ— æ³•è·å–æœ€è¿‘æäº¤çš„æ–‡ä»¶: {e}\")\n        sys.exit(1)\n    \n    if len(committed_files) == 0:\n        print(\"é”™è¯¯ï¼šåœ¨å·¥ä½œåŒºå’Œæœ€è¿‘ä¸€æ¬¡æäº¤ä¸­éƒ½æ²¡æœ‰æ‰¾åˆ°åšå®¢ .md æ–‡ä»¶\")\n        sys.exit(1)\n    elif len(committed_files) == 1:\n        print(f\"âœ… ä½¿ç”¨æœ€è¿‘æäº¤ä¸­çš„æ–‡ä»¶: {committed_files[0].name}\")\n        return committed_files[0]\n    else:\n        print(f\"é”™è¯¯ï¼šæœ€è¿‘ä¸€æ¬¡æäº¤ä¸­æ‰¾åˆ°å¤šä¸ª .md æ–‡ä»¶ï¼š\")\n        for file in committed_files:\n            print(f\"  - {file.relative_to(root_dir)}\")\n        sys.exit(1)\n\n\ndef extract_title_from_markdown(file_path: Path) -> str:\n    \"\"\"\n    ä»markdownæ–‡ä»¶æå–æ ‡é¢˜\n    \n    Args:\n        file_path: markdownæ–‡ä»¶è·¯å¾„\n        \n    Returns:\n        str: æ–‡ç« æ ‡é¢˜\n        \n    Raises:\n        SystemExit: å¦‚æœæ— æ³•è§£ææ–‡ä»¶æˆ–ç¼ºå°‘æ ‡é¢˜\n    \"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            post = frontmatter.load(f)\n    except Exception as e:\n        print(f\"é”™è¯¯ï¼šæ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}\")\n        sys.exit(1)\n    \n    title = post.metadata.get('title')\n    if not title:\n        print(f\"é”™è¯¯ï¼šæ–‡ä»¶ {file_path} çš„frontmatterä¸­ç¼ºå°‘ 'title' å­—æ®µ\")\n        sys.exit(1)\n    \n    return str(title)\n\n\ndef generate_slug_with_llm(title: str) -> str:\n    \"\"\"\n    è°ƒç”¨LLMç”Ÿæˆslug\n    \n    Args:\n        title: æ–‡ç« æ ‡é¢˜\n        \n    Returns:\n        str: ç”Ÿæˆçš„slug\n        \n    Raises:\n        SystemExit: å¦‚æœLLMè°ƒç”¨å¤±è´¥\n    \"\"\"\n    prompt = f\"Generate a URL slug for this article title: {title}\\n\\nRequirements:\\n- Only use lowercase letters, numbers, and hyphens (-)\\n- Maximum 50 characters\\n- Be concise and reflect the main topic\\n- Return only the slug string\\n\\nSlug:\"\n\n    try:\n        client = create_llm_client()\n        \n        # è°ƒç”¨LLMç”Ÿæˆslug\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        response_obj = client.chat_completion(\n            messages=messages,\n            temperature=0.3  # é™ä½éšæœºæ€§ï¼Œç¡®ä¿è¾“å‡ºç¨³å®š\n        )\n        \n        response = response_obj.choices[0].message.content or \"\"\n        \n        # æ¸…ç†å“åº”ï¼Œå»é™¤å¯èƒ½çš„å‰ç¼€å’Œåç¼€\n        slug = response.strip()\n        if slug.startswith('slug:'):\n            slug = slug[5:].strip()\n        if slug.startswith('Slug:'):\n            slug = slug[5:].strip()\n        \n        # éªŒè¯ç”Ÿæˆçš„slugæ ¼å¼\n        if not slug:\n            print(f\"é”™è¯¯ï¼šLLMè¿”å›ç©ºå“åº”\")\n            sys.exit(1)\n        \n        if not all(c.islower() or c.isdigit() or c == '-' for c in slug):\n            print(f\"é”™è¯¯ï¼šç”Ÿæˆçš„slugåŒ…å«éæ³•å­—ç¬¦: {slug}\")\n            print(f\"éæ³•å­—ç¬¦: {[c for c in slug if not (c.islower() or c.isdigit() or c == '-')]}\")\n            sys.exit(1)\n            \n        return slug\n        \n    except Exception as e:\n        print(f\"é”™è¯¯ï¼šLLMè°ƒç”¨å¤±è´¥: {e}\")\n        import traceback\n        traceback.print_exc()\n        sys.exit(1)\n\n\ndef rename_blog_file(old_path: Path, slug: str) -> Path:\n    \"\"\"\n    æ ¹æ®slugé‡å‘½åæ–‡ä»¶\n    \n    Args:\n        old_path: åŸæ–‡ä»¶è·¯å¾„\n        slug: æ–°çš„slug\n        \n    Returns:\n        Path: æ–°æ–‡ä»¶è·¯å¾„\n        \n    Raises:\n        SystemExit: å¦‚æœé‡å‘½åå¤±è´¥\n    \"\"\"\n    new_filename = f\"{slug}.md\"\n    new_path = old_path.parent / new_filename\n    \n    # æ£€æŸ¥æ–°æ–‡ä»¶åæ˜¯å¦å·²å­˜åœ¨\n    if new_path.exists():\n        print(f\"é”™è¯¯ï¼šç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨: {new_path}\")\n        sys.exit(1)\n    \n    try:\n        old_path.rename(new_path)\n        return new_path\n    except Exception as e:\n        print(f\"é”™è¯¯ï¼šé‡å‘½åæ–‡ä»¶å¤±è´¥: {e}\")\n        sys.exit(1)\n\n\ndef generate_blog_slug() -> None:\n    \"\"\"\n    ç”Ÿæˆåšå®¢æ–‡ç« slugçš„ä¸»å‡½æ•°\n    \"\"\"\n    print(\"ğŸš€ å¼€å§‹è‡ªåŠ¨ç”Ÿæˆæ–‡ç« slug...\")\n    \n    # 1. æ‰¾åˆ°æ­£åœ¨ä¿®æ”¹çš„åšå®¢æ–‡ä»¶\n    print(\"ğŸ“ æ£€æµ‹æ­£åœ¨ä¿®æ”¹çš„åšå®¢æ–‡ä»¶...\")\n    blog_file = find_modified_blog_file()\n    print(f\"âœ… æ‰¾åˆ°æ–‡ä»¶: {blog_file.name}\")\n    \n    # 2. æå–æ–‡ç« æ ‡é¢˜\n    print(\"ğŸ“ æå–æ–‡ç« æ ‡é¢˜...\")\n    title = extract_title_from_markdown(blog_file)\n    print(f\"âœ… æ–‡ç« æ ‡é¢˜: {title}\")\n    \n    # 3. ç”Ÿæˆslug\n    print(\"ğŸ¤– è°ƒç”¨LLMç”Ÿæˆslug...\")\n    slug = generate_slug_with_llm(title)\n    print(f\"âœ… ç”Ÿæˆçš„slug: {slug}\")\n    \n    # 4. é‡å‘½åæ–‡ä»¶\n    print(\"ğŸ“ é‡å‘½åæ–‡ä»¶...\")\n    new_path = rename_blog_file(blog_file, slug)\n    print(f\"âœ… æ–‡ä»¶å·²é‡å‘½å: {blog_file.name} -> {new_path.name}\")\n    \n    print(\"ğŸ‰ slugç”Ÿæˆå®Œæˆï¼\")\n\n\nif __name__ == \"__main__\":\n    generate_blog_slug()"
        },
        "modified_files": {
            "writing_helper/main.py": "#!/usr/bin/env python3\n\"\"\"\nWriting Helper - åŸºäºå¤§æ¨¡å‹çš„å†™ä½œåŠ©æ‰‹\n\nä¸»å…¥å£æ–‡ä»¶ï¼Œæ ¹æ®å‘½ä»¤è¡Œå‚æ•°è°ƒç”¨å¯¹åº”çš„åŠŸèƒ½æ¨¡å—\n\"\"\"\n\nimport sys\nfrom pathlib import Path\n\n# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥æ¨¡å—\ncurrent_dir = Path(__file__).parent\nsys.path.append(str(current_dir))\n\nfrom slug_generator import generate_blog_slug\n\n\ndef main():\n    \"\"\"ä¸»å‡½æ•° - å†™ä½œåŠ©æ‰‹å…¥å£\"\"\"\n    # ç›®å‰åªæœ‰slugç”ŸæˆåŠŸèƒ½ï¼Œç›´æ¥è°ƒç”¨\n    # æœªæ¥å¯ä»¥æ ¹æ®å‘½ä»¤è¡Œå‚æ•°æ‰©å±•å…¶ä»–åŠŸèƒ½\n    generate_blog_slug()\n\n\nif __name__ == \"__main__\":\n    main()"
        },
        "deleted_files": [
            "writing_helper/writing_helper/__init__.py",
            "writing_helper/writing_helper/llm.py",
            "writing_helper/writing_helper/main.py"
        ]
    }
]
