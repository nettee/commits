[
    {
        "commit_message": "design",
        "added_files": {
            "gemini-cli-proxy/plan.md": "# Gemini CLI Proxy 设计文档\n\n## 📋 项目概述\n\n### 🎯 项目目标\n将命令行工具 `gemini` 包装成 OpenAI 兼容的 HTTP API 服务，使其能够与 Cherry Studio 等 OpenAI 客户端无缝集成。\n\n### 🔧 核心功能\n- **OpenAI API 兼容**：实现 `/v1/chat/completions` 端点\n- **命令行集成**：调用本地 `gemini` 命令行工具\n- **速率限制**：每分钟最多 60 次请求\n- **流式协议兼容（Fake Stream）**：使用 Server-Sent Events 封装一次性结果\n- **现代化分发**：通过 `uv`/`uvx` 分发和运行\n\n### 🚫 范围说明\n本项目定位为 **本地单机运行** 的开源工具，假设用户对运行环境具有完全控制权。为保持简洁，本文档不涵盖以下主题：  \n1. **安全与鉴权**  \n2. **运维与监控**  \n3. **自动化测试与 CI/CD**\n\n## 🏗️ 技术架构\n\n### 📦 技术选型\n\n#### HTTP 框架：FastAPI\n**选择理由**：\n- ✅ **现代化设计**：基于标准 Python 类型提示\n- ✅ **高性能**：基于 Starlette 和 Pydantic，性能优异\n- ✅ **自动文档**：自动生成 OpenAPI/Swagger 文档\n- ✅ **AI 生态原生**：Python 在 AI 领域的原生优势\n- ✅ **MCP 生态协同**：与现有 AI 工具链一致\n\n#### 包管理：UV\n**选择理由**：\n- ✅ **极致速度**：Rust 编写，比 pip 快 10-100 倍\n- ✅ **零安装运行**：uvx 提供类似 npx 的体验\n- ✅ **MCP 生态标准**：AI 开发者普遍使用\n- ✅ **现代化工具链**：项目管理、依赖解析一体化\n\n#### 关键依赖\n\n```toml\ndependencies = [\n  \"fastapi>=0.104.0\",           # HTTP 服务器框架\n  \"uvicorn[standard]>=0.24.0\",  # ASGI 服务器\n  \"click>=8.0.0\",               # CLI 参数解析\n  \"pydantic>=2.0.0\",            # 数据验证\n  \"slowapi>=0.1.9\",             # 速率限制\n]\n```\n\n### 🏛️ 系统架构\n\n```\n┌─────────────────┐    HTTP     ┌─────────────────┐    Command    ┌─────────────────┐\n│                 │  Request    │                 │     Line      │                 │\n│ OpenAI Client   │────────────▶│ FastAPI Server  │──────────────▶│   Gemini CLI    │\n│ (Cherry Studio) │             │                 │               │     Tool        │\n│                 │◀────────────│                 │◀──────────────│                 │\n└─────────────────┘   Response  └─────────────────┘    Output     └─────────────────┘\n```\n\n### 📂 项目结构\n\n```\ngemini-cli-proxy/\n├── src/gemini_cli_proxy/\n│   ├── __init__.py\n│   ├── cli.py              # CLI 入口点\n│   ├── server.py           # FastAPI 服务器\n│   ├── gemini_client.py    # Gemini 命令行客户端\n│   ├── openai_adapter.py   # OpenAI 格式适配器\n│   ├── models.py           # 数据模型定义\n│   └── config.py           # 配置管理\n├── tests/\n│   ├── unit/               # 单元测试\n│   └── integration/        # 集成测试\n├── pyproject.toml          # 项目配置\n├── README.md               # 使用说明\n└── LICENSE                 # 开源协议\n```\n\n### 🔄 数据流设计\n1. **请求接收**：FastAPI 接收 OpenAI 格式的 HTTP 请求\n2. **格式验证**：Pydantic 自动验证请求格式和参数\n3. **速率控制**：中间件检查是否超过限制\n4. **格式转换**：将 OpenAI 格式转换为 Gemini 命令行参数\n5. **异步执行**：使用 asyncio 调用本地命令行工具\n6. **结果解析**：解析命令行输出并格式化\n7. **流式协议封装（可选）**：按需将完整结果拆分成多条 SSE 事件，实现兼容 OpenAI Streaming 协议\n8. **响应返回**：在非流式模式下返回 OpenAI 兼容 JSON；在流式模式下以 `[DONE]` 事件结尾\n\n### 📝 实现细节补充\n- **Token 用量**：由于 `gemini` CLI 工具不提供 Token 消耗信息，为保证数据准确性，本代理服务的 API 响应中将 **省略** `usage` 字段。\n- **“假流式”实现**：当请求参数 `stream=true` 时，代理会将 `gemini` CLI 的完整输出结果 **按行（line-by-line）** 拆分，并封装成符合 OpenAI 规范的 Server-Sent Events (SSE) 流式返回。\n\n## 🔧 核心模块设计\n\n### 1. CLI 入口模块 (`cli.py`)\n**职责**：命令行参数解析和应用启动\n- 使用 Click 处理命令行参数\n- 创建应用实例并启动 uvicorn 服务器\n\n### 2. FastAPI 服务器 (`server.py`)\n**职责**：HTTP 服务和 API 端点\n- 配置 CORS 和中间件\n- 实现 `/v1/chat/completions` 端点\n- 集成速率限制和错误处理\n\n### 3. Gemini 客户端 (`gemini_client.py`)\n**职责**：与 Gemini CLI 工具交互\n- 异步执行命令行工具\n- 处理超时和错误情况\n- 支持假流式拆分（可选）\n\n### 4. OpenAI 适配器 (`openai_adapter.py`)\n**职责**：格式转换和兼容性\n- OpenAI 请求格式 → Gemini 命令行参数\n- Gemini 输出 → OpenAI 响应格式\n- 流式协议封装（Fake Stream）\n\n### 5. 数据模型 (`models.py`)\n**职责**：请求/响应数据验证\n- Pydantic 模型定义\n- 自动数据验证和序列化\n\n### 6. 配置管理 (`config.py`)\n**职责**：应用配置和环境变量\n- 服务器配置（端口、主机等）\n- Gemini CLI 配置（命令路径、超时等）\n- 环境变量支持\n\n## 📡 API 设计\n\n### 端点规范\n\n#### `POST /v1/chat/completions`\n**请求格式**：\n```json\n{\n  \"model\": \"gemini-pro\",\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n  ],\n  \"temperature\": 0.7,\n  \"max_tokens\": 1000,\n  \"stream\": false\n}\n```\n\n**响应格式**：\n```json\n{\n  \"id\": \"chatcmpl-1234567890\",\n  \"object\": \"chat.completion\",\n  \"created\": 1677652288,\n  \"model\": \"gemini-pro\",\n  \"choices\": [{\n    \"index\": 0,\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": \"Hello! How can I help you today?\"\n    },\n    \"finish_reason\": \"stop\"\n  }],\n  \"usage\": {\n    \"prompt_tokens\": 10,\n    \"completion_tokens\": 20,\n    \"total_tokens\": 30\n  }\n}\n```\n\n> **注**：为保证数据准确性，由于 `gemini` CLI 不提供 Token 用量信息，实际响应中将 **省略 `usage` 字段**。\n\n#### `GET /health`\n健康检查端点，返回服务状态。\n\n#### `GET /v1/models`\n返回可用模型列表（可选实现）。\n\n## 🚀 部署和发布\n\n### 项目配置 (`pyproject.toml`)\n\n```toml\n[project]\nname = \"gemini-cli-proxy\"\nversion = \"1.0.0\"\ndescription = \"OpenAI-compatible API wrapper for Gemini CLI\"\nreadme = \"README.md\"\nlicense = { text = \"MIT\" }\nauthors = [\n    { name = \"Your Name\", email = \"your.email@example.com\" }\n]\nkeywords = [\"openai\", \"gemini\", \"api\", \"proxy\", \"cli\"]\nrequires-python = \">=3.8\"\ndependencies = [\n    \"fastapi>=0.104.0\",\n    \"uvicorn[standard]>=0.24.0\",\n    \"click>=8.0.0\",\n    \"pydantic>=2.0.0\",\n    \"slowapi>=0.1.9\",\n]\n\n[project.scripts]\ngemini-cli-proxy = \"gemini_cli_proxy.cli:main\"\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n```\n\n### 发布流程\n\n1. **本地开发**\n   ```bash\n   # 使用 uv 安装依赖\n   uv sync\n   \n   # 开发模式安装\n   uv pip install -e .\n   ```\n\n2. **构建和发布**\n   ```bash\n   # 构建分发包\n   uv build\n   \n   # 发布到 PyPI\n   uv publish\n   ```\n\n3. **用户安装**\n   ```bash\n   uvx gemini-cli-proxy\n   ```\n\n## 🎛️ 配置管理\n\n### 配置层次\n1. **命令行参数**：运行时指定\n2. **环境变量**：`GEMINI_PROXY_*` 前缀\n3. **配置文件**：`.env` 文件支持\n\n### 关键配置项\n- **服务器**：端口、主机、日志级别\n- **Gemini CLI**：命令路径、超时时间\n- **限制**：速率限制、请求大小\n- **并发**：子进程最大并发数\n\n### 环境变量支持\n\n```bash\n# 服务器配置\nGEMINI_PROXY_HOST=127.0.0.1\nGEMINI_PROXY_PORT=7000\n\n# Gemini CLI 配置\nGEMINI_PROXY_TIMEOUT=30.0\nGEMINI_PROXY_RATE_LIMIT=60\n\n# 并发配置\nGEMINI_PROXY_MAX_CONCURRENCY=4\n\n# 日志配置\nGEMINI_PROXY_LOG_LEVEL=info\nGEMINI_PROXY_DEBUG=false\n```\n\n## ⚙️ 并发与性能\n\n- 使用 `asyncio.create_subprocess_exec` 启动 Gemini CLI，避免阻塞事件循环。  \n- 通过 `asyncio.Semaphore` 控制并发子进程数量，默认值为 `min(4, CPU 核心数)`，可由环境变量 `GEMINI_PROXY_MAX_CONCURRENCY` 或 `--max-concurrency` 参数覆盖。  \n- 速率限制依赖 SlowAPI，默认每分钟 60 次请求，可通过 `--rate-limit` 或 `GEMINI_PROXY_RATE_LIMIT` 修改。\n\n## 💥 错误处理设计\n\n| 场景 | HTTP 状态码 | OpenAI `error.type` | 说明 |\n|------|-------------|---------------------|------|\n| 请求参数校验失败 | 400 | invalid_request_error | Pydantic 报错信息 |\n| 触发速率限制 | 429 | rate_limit_error | 返回 `Retry-After` 头 |\n| CLI 超时或非零退出 | 502 | bad_gateway | `stderr` 作为 message |\n| CLI 被取消 / KeyboardInterrupt | 500 | internal_error | - |\n| 未知异常 | 500 | internal_error | 兜底处理 |\n\n所有错误响应均遵循 OpenAI 错误 JSON：\n```json\n{\n  \"error\": {\n    \"message\": \"...\",\n    \"type\": \"invalid_request_error\",\n    \"param\": null,\n    \"code\": \"400\"\n  }\n}\n```\n## 🎯 使用示例\n\n### 安装和启动\n直接运行：\n```bash\nuvx gemini-cli-proxy --port 7000\n```\n\n在项目中启动：\n```bash\nuv run gemini-cli-proxy --port 7000\n```\n\n### Cherry Studio 配置\n1. 启动服务：`uvx gemini-cli-proxy`\n2. 在 Cherry Studio 设置中：\n   - Provider: OpenAI\n   - Base URL: `http://localhost:7000/v1`\n   - API Key: 任意字符串（不验证）\n   - Model: `gemini-pro`\n\n### OpenAI 客户端使用\n```python\nfrom openai import OpenAI\n\n# 初始化客户端\nclient = OpenAI(\n    base_url='http://localhost:7000/v1',\n    api_key='dummy-key'  # 任意字符串，不验证\n)\n\n# 发送聊天请求\nresponse = client.chat.completions.create(\n    model='gemini-pro',\n    messages=[\n        {'role': 'user', 'content': 'Hello!'}\n    ],\n    temperature=0.7,\n    max_tokens=1000\n)\n\n# 获取响应内容\nprint(response.choices[0].message.content)\n```"
        },
        "modified_files": {},
        "deleted_files": []
    }
]
